{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"<p> Synthax - An Experiment </p> <p>Synthax is an effort to explore the less popular and unknown optimizer algorithm used for optimization of neural networks. This whole experiment revolves around comparing two types of optimizers: Gradient Descent and Newton Second Moment Update.</p> <p>This project leverages JAX for stable autodiff and Sympy for Symbolical notation of various Mathematical equations.</p> <p></p> <ul> <li>Introduction</li> <li>Source Code</li> <li>Process</li> <li>[Conclusion][ Conclusion.md]</li> </ul> <p>For more information, visit GitHub.</p>"},{"location":"Conclusion/","title":"Conclusion","text":""},{"location":"Conclusion/#iteration-insight","title":"Iteration Insight","text":"<p>Throughout the experiment in various test scenarios, the optimum cost and iteration cost to reach convergence varied significantly, leading to unexpected conclusions.</p>"},{"location":"Conclusion/#optimization-results","title":"Optimization Results","text":""},{"location":"Conclusion/#normal-init","title":"Normal Init","text":"Algorithm Iteration Cost Early Stopping Newton Moment Update 171 6.9812 Yes Gradient Descent 164 6.9564 Yes <p>Newton Moment Update: Iteration stopped early at 171 due to low cost difference. Gradient Descent: Iteration stopped early at 164 due to low cost difference.</p> <p></p> <p>Conclusion: The optimum values reached in both variants are strikingly similar, with Gradient Descent marginally outperforming Newton Momentum Update by a very small margin. The close convergence of both optimization methods suggests that for the given dataset and hyperparameters, the choice between these methods may not significantly impact the final outcome. It's important to note that the performance comparison may vary based on the specific characteristics of the dataset and the problem at hand. The local optimal value was converged early on in the case of Newton optimization.</p>"},{"location":"Conclusion/#best-init","title":"Best Init","text":"Algorithm Iteration Cost Early Stopping Gradient Descent 173 6.9674 Yes Newton Moment Update 98 6.9327 Yes <p>Newton Moment Update: Iteration stopped early at 98 due to low cost difference. Gradient Descent: Iteration stopped early at 173 due to low cost difference.</p> <p></p> <p>Conclusion: A well-thought-out optimization value works phenomenally well for Newton moment optimization, demonstrating that better initialization can make the optimal value converge faster and with better precision.</p>"},{"location":"Conclusion/#worst-init","title":"Worst Init","text":"Algorithm Iteration Cost Early Stopping Gradient Descent 344 6.9559 Yes Newton Moment Update 162 6.9361 Yes <p>Newton Moment Update: Iteration stopped early at 162 due to low cost difference. Gradient Descent: Iteration stopped early at 344 due to low cost difference.</p> <p></p> <p>Conclusion: This striking difference in performance reveals the nature of Newton second moment itself. As this optimization leverages the information of the Hessian and curvature landscape information of values, the performance enhances, and also the use of the momentum factor makes the early updation possible. JAX optimization nature for Autodiff also plays a vital role in this performance, as it can handle stable complex differentiation scenarios well.</p>"},{"location":"Conclusion/#final-conclusion-tldr-of-experiment","title":"Final Conclusion - TLDR of Experiment","text":"<p>In this experiment, we observed that Newton Second Moment (Newton Momentum) outperformed Gradient Descent in most scenarios. This superior performance can be attributed to several factors:</p> <ol> <li> <p>Stable Autodiff Nature of JAX: Newton's method benefits from a stable autodifferentiation process, making it more robust in capturing higher-order derivatives. This stability allows Newton to converge faster and more reliably compared to Gradient Descent.</p> </li> <li> <p>Leveraging Nature of Curvature: Newton's method leverages information about the curvature of the cost function. This allows it to adapt more effectively to the landscape of the optimization problem, making larger updates in regions with less curvature and smaller updates in regions with higher curvature.</p> </li> <li> <p>Hessian Updation Through Random Matrix Operation: The use of random matrix operations in updating the Hessian matrix contributes to the effectiveness of Newton Second Moment. This introduces variability in the updates, aiding in escaping local minima and exploring the solution space more effectively.</p> </li> <li> <p>Less Dependence on Hyperparameters: Newton Second Moment exhibits less sensitivity to hyperparameters, such as learning rate, compared to traditional Gradient Descent. This is because Newton's method computes the optimal step size based on the Hessian matrix, providing more intrinsic adaptability.</p> </li> <li> <p>Use of Second Moment Order Information for Stability: Incorporating second-moment order information in the optimization process enhances stability. The use of the Hessian matrix and its inverse in Newton's method contributes to the stability of the optimization updates.</p> </li> </ol> <p>However, one major disadvantage of Newton Second Moment is its complexity and dependence on optimal weight and bias initialization. The method requires a well-tailored initialization to avoid divergence or slow convergence, making it less user-friendly and potentially challenging in practical applications.</p> <p>In summary, Newton Second Moment's superior performance stems from its stable autodiff nature, effective leverage of curvature information, random matrix operations, and reduced sensitivity to hyperparameters. Nevertheless, its complexity and dependence on optimal initialization are key considerations in its application.</p>"},{"location":"Introduction/","title":"Introduction","text":"<p>This experiment focuses on the nature of optimization approaches used for deep learning. It creates the model from scratch using JAX and demonstrates the symbolic nature of equations using Sympy.</p>"},{"location":"Introduction/#use-case","title":"Use Case","text":"<p>The main purpose of this project is to understand two variants of optimizers: gradient descent and Newton's second moment update using the Hessian matrix. To compare these, a minimum viable model is produced from scratch based on JAX, and various demonstrations of processes involved in deep learning are explored.</p>"},{"location":"Introduction/#key-highlights","title":"Key Highlights","text":"<ul> <li>Utilization of JAX for efficient numerical optimization.</li> <li>Sympy's symbolic mathematics for clear formulation and demonstration.</li> <li>Testing and comparison of simple gradient descent and Newton's second moment.</li> <li>Using visualization to demonstrate the complex nature of the optimization process.</li> <li>Explore the less popular approach of Newton's second moment update with the more popularly used gradient descent algorithm of optimization.</li> </ul>"},{"location":"Introduction/#gradient-descent-optimization","title":"Gradient Descent Optimization","text":"<ol> <li> <p>Update Rule: In each iteration, the weights (parameters) are updated in the opposite direction of the gradient, aiming to minimize the loss function.</p> </li> <li> <p>Learning Rate:The learning rate determines the step size of each update, influencing the convergence speed and stability of the algorithm.</p> </li> <li> <p>Convergence: Gradient descent continues iterating until the algorithm converges to a minimum, where the gradient becomes zero.</p> </li> <li> <p>Global Minima Challenge: It may get stuck in local minima, especially in non-convex loss landscapes, affecting the global optimality.</p> </li> <li> <p>Computational Efficiency: Efficient for large datasets but might be computationally expensive in high-dimensional spaces.</p> </li> </ol>"},{"location":"Introduction/#newtons-second-moment-update-optimization","title":"Newton's Second Moment Update Optimization","text":"<ol> <li> <p>Hessian Matrix: Utilizes the Hessian matrix, which incorporates second-order information about the loss function, providing a more accurate and efficient optimization direction.</p> </li> <li> <p>Update Rule: Computes the Newton update direction by solving a linear system involving the Hessian and gradient, leading to more precise parameter adjustments.</p> </li> <li> <p>Learning Rate Absence: Newton's method does not require a fixed learning rate, as the update direction is determined directly from the Hessian.</p> </li> <li> <p>Quicker Convergence: Often converges faster than gradient descent, especially in scenarios with strong curvatures or complex loss landscapes.</p> </li> <li> <p>Challenges: Computational cost of computing and inverting the Hessian, and potential numerical instability in ill-conditioned matrices. Requires heavy dependence on parameter initialization.</p> </li> </ol>"},{"location":"Process/","title":"Process","text":""},{"location":"Process/#introduction-to-process","title":"Introduction to process","text":"<p>The whole experiment is divided into three test which is based on three different approaches to explore the experiment</p> <ul> <li>Normal Init - Normal Initialization of parameter</li> <li>Better Init - Better Initialization of parameter</li> <li>Worst Init - Poor Initialization of paramater</li> </ul> <p>The main crux of the exploration behind this experiment is that newton second moment update heavily influence by the initial parameter initialization , the whole experiment is evaluated through:</p> <ul> <li>The convergence precision</li> <li>Number of iteration it took</li> <li>The performance of the optimiser in various situation</li> </ul>"},{"location":"Process/#result-for-each-iteration","title":"Result for each iteration","text":""},{"location":"Process/#normal-init","title":"Normal Init","text":"Python<pre><code>#Normal Initialisation\njax.random.PRNGKey(0)\nw_init = jax.random.normal(key=jax.random.PRNGKey(2), shape=(X.shape[1], 1))\nb_init = jax.random.normal(key=jax.random.PRNGKey(3), shape=(1,))\n</code></pre> <p>Gradient Descent</p> Python<pre><code># Run JAX Optimized Gradient Loop\nstart_time = time.time()\nresult_gd1 = jax_optimized_gradient_loop(X, y, w_init=w_init, b_init=b_init, learning_rate=0.01, patience=2,tolerance=0.001)\nelapsed_time = time.time()-start_time\nprint(f\"Time taken: {elapsed_time} seconds\")\n</code></pre> <p>Newton second moment update</p> Python<pre><code># Run Newton Momentum Update\nstart_time = time.time()\nresult_nm1 = newton_momentum_update(X, y,w_init=w_init,b_init=b_init, alpha=0.01, beta=0.9, num_iterations=1000,patience=2,tolerance=0.001)\nelapsed_time1 = time.time()-start_time\nprint(f\"Time taken: {elapsed_time} seconds\")\n</code></pre> <p>plot_cost_comparison(result_gd1,result_nm1)</p>"},{"location":"Process/#better-init","title":"Better Init","text":"Python<pre><code>#Better initialization of parameters.\n# -features and hyperparams remain same.\nw_init = jnp.zeros((X.shape[1], 1))\nb_init = jnp.zeros((1,))\n</code></pre> Python<pre><code># Run JAX Optimized Gradient Loop\nstart_time = time.time()\nresult_gd2 = jax_optimized_gradient_loop(X, y, w_init, b_init, learning_rate=0.01, patience=2,tolerance=0.001)\nelapsed_time2 = time.time()-start_time\nprint(f\"Time taken: {elapsed_time} seconds\")\n\n```python\n# Run Newton Momentum Update\nstart_time = time.time()\nresult_nm2 = newton_momentum_update(X, y,w_init=w_init,b_init=b_init, alpha=0.01, beta=0.9, num_iterations=1000,patience=2,tolerance=0.001)\nelapsed_time3 = time.time()-start_time\nprint(f\"Time taken: {elapsed_time} seconds\")\n</code></pre> Python<pre><code>plot_cost_comparison(result_gd2,result_nm2)\n</code></pre>"},{"location":"Process/#worst-init","title":"Worst Init","text":"Python<pre><code>#Worst initialization of parameters.\n# -features and hyperparams remain same.\nw_init = jax.random.normal(key=jax.random.PRNGKey(4), shape=(X.shape[1], 1))\nb_init = jnp.array([160.0])\n</code></pre> Python<pre><code># Run JAX Optimized Gradient Loop\nstart_time = time.time()\nresult_gd3 = jax_optimized_gradient_loop(X, y, w_init, b_init, learning_rate=0.01, patience=2,tolerance=0.001)\nelapsed_time4 = time.time()-start_time\nprint(f\"Time taken: {elapsed_time} seconds\")\n</code></pre> Python<pre><code># Run Newton Momentum Update\nstart_time = time.time()\nresult_nm3 = newton_momentum_update(X, y,w_init,b_init, alpha=0.01, beta=0.9, num_iterations=1000,patience=2,tolerance=0.001)\nelapsed_time5 = time.time()-start_time\nprint(f\"Time taken: {elapsed_time} seconds\")\n</code></pre> Python<pre><code>plot_cost_comparison(result_gd3,result_nm3)\n</code></pre>"},{"location":"SourceCode/","title":"Source Code","text":""},{"location":"SourceCode/#data-initialization","title":"Data Initialization","text":"<p>JAX compartible data Initialisation for experiment , feel free to change params as your preference!</p> <p>To initialize data for the problem set. Feel free to copy and use it:</p> Python<pre><code># Set a random seed for reproducibility\njax.random.PRNGKey(0)\n\n# Number of data points\nnum_points = 1000\n\n# Features\nX = jax.random.normal(key=jax.random.PRNGKey(0), shape=(num_points, 2))\n\n# True coefficients\ntrue_coefficients = jnp.array([2.5, -1.0])\n\n# True bias\ntrue_bias = 5.0\n\n# Generate target values with some noise\ny = jnp.dot(X, true_coefficients) + true_bias + jax.random.normal(key=jax.random.PRNGKey(1), shape=(num_points, 1)) * 0.5\n\n# Print shapes for verification\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)\n</code></pre>"},{"location":"SourceCode/#functions","title":"Functions","text":"<p>Functions built up from scratch in JAX compartible format for the process</p> <p>JAX platform optimized cost function to obtain cost after each epoch</p> Python<pre><code>def cost(X, w, b, y):\n    \"\"\"\n    Compute the mean squared error for linear regression.\n\n    Parameters:\n    - X: Input feature matrix.\n    - w: Weight matrix.\n    - b: Bias vector.\n    - y: Target feature vector.\n\n    Returns:\n    - Mean squared error between predicted and actual target values.\n    \"\"\"\n    # Predicted values using linear regression\n    y_pred = jnp.dot(X, w) + b\n\n    # Mean squared error calculation\n    mse = jnp.mean((y_pred - y)**2)\n\n    return mse\n</code></pre> <p>JAX platform optimized gradient descent optimizer</p> Python<pre><code>def gradient_descent(X, w, b, y, learning_rate=0.01):\n    \"\"\"\n    Perform one step of gradient descent optimization for linear regression.\n\n    Parameters:\n    - X: Input feature matrix.\n    - w: Weight matrix.\n    - b: Bias vector.\n    - y: Target feature vector.\n    - learning_rate: Step size for weight and bias updates.\n\n    Returns:\n    - Updated weight matrix (w) and bias vector (b) after one optimization step.\n    \"\"\"\n    # Compute gradients with respect to weights and biases\n    grad_w = jax.grad(cost, argnums=1)(X, w, b, y)\n    grad_b = jax.grad(cost, argnums=2)(X, w, b, y)\n\n    # Update weights and biases using the gradient and learning rate\n    w -= learning_rate * grad_w\n    b -= learning_rate * grad_b\n\n    return w, b\n</code></pre> <p>JAX platform optimized Newton second moment update optimizer</p> Python<pre><code>def newton_momentum_update(X, y, w_init, b_init, alpha=0.01, beta=0.9,\n                            num_iterations=1000, cost_display_interval=20,\n                            patience=5, tolerance=1e-6):\n    \"\"\"\n    Perform Newton Momentum update for linear regression.\n\n    Parameters:\n    - X: Input feature matrix.\n    - y: Target feature vector.\n    - w_init: Initial weight matrix. If None, it is initialized with zeros.\n    - b_init: Initial bias vector. If None, it is initialized with zeros.\n    - alpha: Learning rate.\n    - beta: Momentum parameter.\n    - num_iterations: Number of iterations.\n    - cost_display_interval: Interval for displaying cost during training.\n    - patience: Number of consecutive iterations with cost difference less than tolerance to trigger early stopping.\n    - tolerance: Tolerance for cost difference to trigger early stopping.\n\n    Returns:\n    - Dictionary containing the weight matrix (w), bias vector (b), and cost value for each iteration.\n    \"\"\"\n    # Example usage:\n    # result_newton = newton_momentum_update(X, y, w_init=w, b_init=b)\n    # Access results using result_newton['w'], result_newton['b'], result_newton['cost']\n\n    # Initialize parameters\n    w = w_init if w_init is not None else jnp.zeros((X.shape[1], 1))\n    b = b_init if b_init is not None else jnp.zeros((1))\n\n    # Initialize variables for early stopping\n    consecutive_low_difference = 0\n\n    # Initialize dictionary to store results\n    optimization_results = {'w': [], 'b': [], 'cost': []}\n\n    # Perform Newton Momentum update\n    for i in range(num_iterations):\n        # Compute cost\n        y_pred = jnp.dot(X, w) + b\n        cost = jnp.mean((y_pred - y)**2)\n\n        # Compute gradient\n        gradient_w = 2 * jnp.dot(X.T, (y_pred - y))\n        gradient_b = 2 * jnp.sum(y_pred - y)\n\n        # Compute Hessian\n        hessian_w = 2 * jnp.dot(X.T, X)\n        hessian_b = 2 * X.shape[0]\n\n        # Update direction\n        update_direction_w = jnp.linalg.solve(hessian_w, -gradient_w)\n        update_direction_b = -gradient_b / hessian_b\n\n        # Update with momentum\n        if i == 0:\n            momentum_w = jnp.zeros_like(update_direction_w)\n            momentum_b = 0.0\n        else:\n            momentum_w = beta * momentum_w + (1 - beta) * update_direction_w\n            momentum_b = beta * momentum_b + (1 - beta) * update_direction_b\n\n        # Parameter update\n        w += alpha * momentum_w\n        b += alpha * momentum_b\n\n        # Save results\n        optimization_results['w'].append(w.copy())\n        optimization_results['b'].append(b.copy())\n        optimization_results['cost'].append(cost)\n\n        # Print intermediate results at specified interval\n        if i % cost_display_interval == 0:\n            print(f\"Iteration {i+1} - Newton Moment Update Cost: {cost}\")\n\n        # Early stopping check\n        if i &gt; 0:\n            cost_difference = abs(previous_cost - cost)\n            if cost_difference &lt; tolerance:\n                consecutive_low_difference += 1\n            else:\n                consecutive_low_difference = 0\n\n            if consecutive_low_difference &gt;= patience:\n                print(f\"Early stopping at iteration {i+1} due to low cost difference.\")\n                break\n\n        # Save current cost for the next iteration\n        previous_cost = cost\n\n    return optimization_results\n</code></pre> <p>Main function to run the gradient descent loop</p> Python<pre><code>def jax_optimized_gradient_loop(X, y, w_init, b_init, learning_rate=0.01,\n                                 patience=5, cost_display_interval=20,\n                                  tolerance=1e-6, epochs=1000):\n    \"\"\"\n    Perform JAX platform optimized gradient descent optimization loop for linear regression.\n\n    Parameters:\n    - X: Input feature matrix.\n    - y: Target feature vector.\n    - w_init: Initial weight matrix.\n    - b_init: Initial bias vector.\n    - learning_rate: Step size for weight and bias updates.\n    - patience: Number of consecutive iterations with cost difference less than tolerance to trigger early stopping.\n    - cost_display_interval: Interval for displaying cost during training.\n    - tolerance: Tolerance for cost difference to trigger early stopping.\n    - epochs: Number of iterations for the optimization loop.\n\n    Returns:\n    - Dictionary containing the weight matrix (w), bias vector (b), and cost value for each iteration.\n    \"\"\"\n    # Example usage:\n    # result = jax_optimized_gradient_loop(X, y, w_init, b_init)\n    # Access results using result['w'], result['b'], result['cost']\n    # Initialize parameters\n    w = w_init.copy()\n    b = b_init.copy()\n\n    # Initialize variables for early stopping\n    consecutive_low_difference = 0\n\n    # Initialize dictionary to store results\n    optimization_results = {'w': [], 'b': [], 'cost': []}\n\n    # Perform gradient descent optimization loop\n    for i in range(epochs):\n        # Update weights and biases using the gradient descent function\n        w, b = gradient_descent(X, w, b, y, learning_rate)\n\n        # Compute cost after each epoch\n        cost_value = cost(X, w, b, y)\n\n        # Save results\n        optimization_results['w'].append(w.copy())\n        optimization_results['b'].append(b.copy())\n        optimization_results['cost'].append(cost_value)\n\n        # Display cost at specified interval\n        if i % cost_display_interval == 0:\n            print(f\"Iteration {i+1} - Gradient Descent Cost: {cost_value}\")\n\n        # Early stopping check\n        if i &gt; 0:\n            cost_difference = abs(previous_cost - cost_value)\n            if cost_difference &lt; tolerance:\n                consecutive_low_difference += 1\n            else:\n                consecutive_low_difference = 0\n\n            if consecutive_low_difference &gt;= patience:\n                print(f\"Early stopping at iteration {i+1} due to low cost difference.\")\n                break\n\n        # Save current cost for the next iteration\n        previous_cost = cost_value\n\n    return optimization_results\n</code></pre>"},{"location":"SourceCode/#visualization","title":"Visualization","text":"<p>For Visualising the change in the values with respect to both optimizer</p> Python<pre><code>def plot_cost_comparison(dict1, dict2):\n     # Create dataframes from dictionaries\ndf1 = pd.DataFrame({'Iteration': range(1, len(dict1['cost']) + 1)\n        , 'Cost': dict1['cost'], 'Optimizer': 'Gradient Descent'})\ndf2 = pd.DataFrame({'Iteration': range(1, len(dict2['cost']) + 1),\n         'Cost': dict2['cost'], 'Optimizer': 'Newton second moment update'})\n\n    # Concatenate dataframes\n    df = pd.concat([df1, df2])\n\n    # Plot using Seaborn\n    sns.set(style=\"whitegrid\")\n    g = sns.FacetGrid(df, col=\"Optimizer\", height=6, aspect=1)\n    g.map(plt.plot, \"Iteration\", \"Cost\", marker=\"o\", color=\"b\")\n    g.fig.tight_layout(pad=2.0)\n\n\n    # Set custom title for the facet grid\n    plt.subplots_adjust(top=0.9, hspace=0.5)  # Adjust as needed\n    g.fig.suptitle(\"Cost Comparison between Gradient descent and Newton second moment update\")\n\n    # Show the plot\n    plt.show()\n</code></pre>"}]}